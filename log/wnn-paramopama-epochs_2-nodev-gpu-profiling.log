/opt/miniconda2/lib/python2.7/site-packages/theano/gpuarray/dnn.py:135: UserWarning: Your cuDNN version is more recent than Theano. If you encounter problems, try updating Theano or downgrading cuDNN to version 5.1.
  warnings.warn("Your cuDNN version is more recent than "
Using cuDNN version 6021 on context None
Mapped name None to device cuda: GeForce GTX 1080 Ti (0000:03:00.0)
{"timestamp": "2017/07/18 17:22:08", "module": "__main__", "level": "INFO", "message": "T(hidden_activation_function='tanh', word_embedding=u'/data/ner/ptwiki_cetenfolha_cetempublico.trunk.voc5.100wv.ctw5.sample1e_1', start_symbol='</s>', shuffle=True, end_symbol='</s>', seed=1, num_epochs=2, char_window_size=5, word_emb_size=100, decay=u'NORMAL', char_emb_size=10, lr=0.01, test=None, hidden_size=300, word_filters=[u'data.Filters.TransformLowerCaseFilter', u'data.Filters.TransformNumberToZeroFilter'], word_lexicon=None, conv_size=50, batch_size=8, train=u'/data/ner/4-Paramopama_corpus.txt_train.txt', word_window_size=5, normFactor=None, adagrad=True, normalization=u'minmax', char_lexicon=u'/data/ner/char_lexicon.txt', dev=None, label_file=u'/data/ner/labels.txt')"}
{"timestamp": "2017/07/18 17:22:08", "module": "__main__", "level": "INFO", "message": "floatX=float32"}
{"timestamp": "2017/07/18 17:22:08", "module": "__main__", "level": "INFO", "message": "device=cuda"}
{"timestamp": "2017/07/18 17:22:08", "module": "__main__", "level": "INFO", "message": "Loading word filters..."}
{"timestamp": "2017/07/18 17:22:08", "module": "__main__", "level": "INFO", "message": "Usando o filtro: data.Filters TransformLowerCaseFilter"}
{"timestamp": "2017/07/18 17:22:08", "module": "__main__", "level": "INFO", "message": "Usando o filtro: data.Filters TransformNumberToZeroFilter"}
{"timestamp": "2017/07/18 17:22:08", "module": "__main__", "level": "INFO", "message": "Loading word embedding..."}
{"timestamp": "2017/07/18 17:22:37", "module": "__main__", "level": "INFO", "message": "Loading char lexicon..."}
{"timestamp": "2017/07/18 17:22:37", "module": "__main__", "level": "INFO", "message": "Loading label lexicon..."}
{"timestamp": "2017/07/18 17:22:37", "module": "data.Lexicon", "level": "WARNING", "message": "The unknown symbol of the lexicon 'label_lexicon' wasn't defined."}
{"timestamp": "2017/07/18 17:22:37", "module": "__main__", "level": "INFO", "message": "Normalizing word embedding: minmax"}
{"timestamp": "2017/07/18 17:22:37", "module": "__main__", "level": "INFO", "message": "Size of word lexicon is 453991 and word embedding size is 100"}
{"timestamp": "2017/07/18 17:22:37", "module": "__main__", "level": "INFO", "message": "Reading training examples"}
{"timestamp": "2017/07/18 17:23:02", "module": "data.TokenDatasetReader", "level": "INFO", "message": "Number of tokens read: 290958"}
{"timestamp": "2017/07/18 17:23:02", "module": "data.BatchIterator", "level": "INFO", "message": "Number of examples: 290958"}
{"timestamp": "2017/07/18 17:23:02", "module": "data.BatchIterator", "level": "INFO", "message": "Number of batches: 36370"}
{"timestamp": "2017/07/18 17:23:02", "module": "data.BatchIterator", "level": "INFO", "message": "BatchSize: 8"}
{"timestamp": "2017/07/18 17:23:02", "module": "__main__", "level": "INFO", "message": "Training algorithm: Adagrad"}
{"timestamp": "2017/07/18 17:23:02", "module": "__main__", "level": "INFO", "message": "Compiling the network..."}
{"timestamp": "2017/07/18 17:23:24", "module": "__main__", "level": "INFO", "message": "Training..."}
{"timestamp": "2017/07/18 17:23:24", "module": "model.BasicModel", "level": "INFO", "message": {"learn_rate": [0.01], "epoch": 0, "iteration": 0}}
{"timestamp": "2017/07/18 17:24:13", "module": "model.BasicModel", "level": "INFO", "message": {"name": "LossTrain", "iteration": 36370, "subtype": "train", "epoch": 0, "values": {"loss": 0.17756223201412763, "accumLoss": 51663.151902366546, "numExamples": 290958}, "type": "metric"}}
{"timestamp": "2017/07/18 17:24:13", "module": "model.BasicModel", "level": "INFO", "message": {"name": "AccTrain", "iteration": 36370, "subtype": "train", "epoch": 0, "values": {"accumAccuracy": 274301.0, "numExamples": 290958, "accuracy": 0.9427511874566089}, "type": "metric"}}
{"timestamp": "2017/07/18 17:24:13", "module": "model.BasicModel", "level": "INFO", "message": {"duration": 49, "subtype": "training", "epoch": 0, "type": "duration", "iteration": 36370}}
{"timestamp": "2017/07/18 17:24:13", "module": "model.BasicModel", "level": "INFO", "message": {"learn_rate": [0.01], "epoch": 1, "iteration": 36370}}
{"timestamp": "2017/07/18 17:25:02", "module": "model.BasicModel", "level": "INFO", "message": {"name": "LossTrain", "iteration": 72740, "subtype": "train", "epoch": 1, "values": {"loss": 0.08185493761710068, "accumLoss": 23816.34893919638, "numExamples": 290958}, "type": "metric"}}
{"timestamp": "2017/07/18 17:25:02", "module": "model.BasicModel", "level": "INFO", "message": {"name": "AccTrain", "iteration": 72740, "subtype": "train", "epoch": 1, "values": {"accumAccuracy": 283202.0, "numExamples": 290958, "accuracy": 0.9733432316691756}, "type": "metric"}}
{"timestamp": "2017/07/18 17:25:02", "module": "model.BasicModel", "level": "INFO", "message": {"duration": 49, "subtype": "training", "epoch": 1, "type": "duration", "iteration": 72740}}
{"timestamp": "2017/07/18 17:25:02", "module": "__main__", "level": "INFO", "message": "Done!"}
Function profiling
==================
  Message: /home/eraldo/lia-pln-deeplearning/model/BasicModel.py:68
  Time in 72739 calls to Function.__call__: 8.634016e+01s
  Time in Function.fn.__call__: 8.301838e+01s (96.153%)
  Time in thunks: 7.678292e+01s (88.931%)
  Total compile time: 1.356649e+01s
    Number of Apply nodes: 119
    Theano Optimizer time: 8.490441e-01s
       Theano validate time: 3.420758e-02s
    Theano Linker time (includes C, CUDA code generation/compiling): 1.268591e+01s
       Import time 6.002665e-03s
       Node make_thunk time 1.267944e+01s
           Node GpuElemwise{tanh,no_inplace}(GpuReshape{3}.0) time 1.397602e+00s
           Node GpuElemwise{Composite{(EQ(i0, i1) * i2)}}[(0, 1)]<gpuarray>(InplaceGpuDimShuffle{0,x,1}.0, GpuElemwise{tanh,no_inplace}.0, InplaceGpuDimShuffle{0,x,1}.0) time 1.379394e+00s
           Node GpuElemwise{Composite{(i0 * (i1 - sqr(tanh(i2))))}}[(0, 0)]<gpuarray>(GpuReshape{2}.0, GpuArrayConstant{[[ 1.]]}, GpuElemwise{Add}[(0, 0)]<gpuarray>.0) time 9.976521e-01s
           Node GpuElemwise{Composite{(i0 * i1 * (i2 / (i3 + sqrt((i4 + i5)))))}}[(0, 2)]<gpuarray>(GpuArrayConstant{[[-1.]]}, InplaceGpuDimShuffle{x,x}.0, GpuReshape{2}.0, GpuArrayConstant{[[  1.00000001e-10]]}, GpuAdvancedSubtensor1.0, GpuElemwise{sqr,no_inplace}.0) time 9.920650e-01s
           Node GpuElemwise{Composite{(i0 * (i1 - sqr(i2)))}}[(0, 0)]<gpuarray>(GpuDot22.0, GpuArrayConstant{[[ 1.]]}, GpuElemwise{Composite{tanh((i0 + i1))}}[(0, 0)]<gpuarray>.0) time 9.838259e-01s

Time in all call to theano.grad() 7.505751e-02s
Time since theano import 177.239s
Class
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Class name>
  26.0%    26.0%      19.931s       3.04e-05s     C   654651       9   theano.gpuarray.blas.GpuDot22
  22.7%    48.7%      17.435s       8.88e-06s     C   1963953      27   theano.gpuarray.elemwise.GpuElemwise
  18.7%    67.3%      14.332s       3.94e-05s     C   363695       5   theano.gpuarray.elemwise.GpuCAReduceCuda
   9.2%    76.5%       7.029s       2.42e-05s     C   290956       4   theano.gpuarray.subtensor.GpuAdvancedSubtensor1
   7.9%    84.4%       6.062s       2.08e-05s     C   290956       4   theano.gpuarray.subtensor.GpuAdvancedIncSubtensor1_dev20
   4.0%    88.3%       3.042s       6.97e-06s     C   436434       6   theano.gpuarray.basic_ops.GpuFromHost
   2.6%    90.9%       1.995s       2.74e-06s     C   727390      10   theano.gpuarray.basic_ops.GpuReshape
   2.0%    92.9%       1.522s       6.97e-06s     C   218217       3   theano.gpuarray.basic_ops.HostFromGpu
   1.7%    94.6%       1.317s       1.29e-06s     C   1018346      14   theano.gpuarray.elemwise.GpuDimShuffle
   1.6%    96.2%       1.203s       1.65e-05s     C    72739       1   theano.gpuarray.basic_ops.GpuJoin
   1.4%    97.6%       1.063s       1.46e-05s     C    72739       1   theano.gpuarray.nnet.GpuCrossentropySoftmaxArgmax1HotWithBias
   0.8%    98.4%       0.649s       8.93e-06s     C    72739       1   theano.gpuarray.nnet.GpuCrossentropySoftmax1HotWithBiasDx
   0.5%    98.9%       0.371s       4.64e-07s     C   800129      11   theano.tensor.elemwise.Elemwise
   0.4%    99.3%       0.295s       4.06e-07s     C   727390      10   theano.compile.ops.Shape_i
   0.3%    99.6%       0.235s       3.59e-07s     C   654651       9   theano.tensor.opt.MakeVector
   0.2%    99.8%       0.174s       2.40e-06s     C    72739       1   theano.gpuarray.basic_ops.GpuSplit
   0.1%   100.0%       0.099s       1.36e-06s     C    72739       1   theano.tensor.basic.Argmax
   0.0%   100.0%       0.029s       1.96e-07s     C   145478       2   theano.gpuarray.basic_ops.GpuContiguous
   ... (remaining 0 Classes account for   0.00%(0.00s) of the runtime)

Ops
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Op name>
  26.0%    26.0%      19.931s       3.04e-05s     C     654651        9   GpuDot22
  16.5%    42.5%      12.679s       5.81e-05s     C     218217        3   GpuCAReduceCuda{add}{0}
   9.2%    51.6%       7.029s       2.42e-05s     C     290956        4   GpuAdvancedSubtensor1
   7.9%    59.5%       6.062s       2.08e-05s     C     290956        4   GpuAdvancedIncSubtensor1_dev20{inplace=True, set_instead_of_inc=False}
   5.4%    64.9%       4.118s       9.44e-06s     C     436434        6   GpuElemwise{Composite{(i0 - (i1 * (i2 / (i3 + sqrt(i4)))))}}[(0, 0)]<gpuarray>
   4.3%    69.2%       3.337s       7.65e-06s     C     436434        6   GpuElemwise{Composite{(i0 + sqr(i1))}}[(0, 0)]<gpuarray>
   4.0%    73.2%       3.042s       6.97e-06s     C     436434        6   GpuFromHost<None>
   2.3%    75.5%       1.742s       3.42e-06s     C     509173        7   GpuReshape{2}
   2.0%    77.4%       1.522s       6.97e-06s     C     218217        3   HostFromGpu(gpuarray)
   1.9%    79.4%       1.489s       1.02e-05s     C     145478        2   GpuElemwise{Composite{(i0 * i1 * (i2 / (i3 + sqrt((i4 + i5)))))}}[(0, 2)]<gpuarray>
   1.8%    81.2%       1.417s       9.74e-06s     C     145478        2   GpuElemwise{Add}[(0, 0)]<gpuarray>
   1.7%    82.9%       1.298s       8.92e-06s     C     145478        2   GpuElemwise{sqr,no_inplace}
   1.6%    84.5%       1.203s       1.65e-05s     C     72739        1   GpuJoin
   1.4%    85.9%       1.063s       1.46e-05s     C     72739        1   GpuCrossentropySoftmaxArgmax1HotWithBias
   1.3%    87.2%       1.003s       1.38e-05s     C     72739        1   GpuCAReduceCuda{maximum}{1}
   1.0%    88.2%       0.766s       1.05e-05s     C     72739        1   GpuElemwise{tanh,no_inplace}
   1.0%    89.2%       0.763s       1.05e-05s     C     72739        1   GpuElemwise{Composite{(EQ(i0, i1) * i2)}}[(0, 1)]<gpuarray>
   0.9%    90.1%       0.729s       1.00e-05s     C     72739        1   GpuElemwise{Composite{(i0 * (i1 - sqr(tanh(i2))))}}[(0, 0)]<gpuarray>
   0.8%    91.0%       0.650s       8.94e-06s     C     72739        1   GpuCAReduceCuda{add}
   0.8%    91.8%       0.649s       8.93e-06s     C     72739        1   GpuCrossentropySoftmax1HotWithBiasDx
   ... (remaining 29 Ops account for   8.19%(6.29s) of the runtime)

Apply
------
<% time> <sum %> <apply time> <time per call> <#call> <id> <Apply name>
  14.7%    14.7%      11.293s       1.55e-04s   72739   107   GpuCAReduceCuda{add}{0}(GpuElemwise{Composite{(i0 * (i1 - sqr(tanh(i2))))}}[(0, 0)]<gpuarray>.0)
   5.3%    20.0%       4.084s       5.61e-05s   72739   108   GpuDot22(InplaceGpuDimShuffle{1,0}.0, GpuElemwise{Composite{(i0 * (i1 - sqr(tanh(i2))))}}[(0, 0)]<gpuarray>.0)
   5.1%    25.1%       3.920s       5.39e-05s   72739    79   GpuDot22(GpuCrossentropySoftmax1HotWithBiasDx.0, InplaceGpuDimShuffle{1,0}.0)
   3.2%    28.4%       2.495s       3.43e-05s   72739   116   GpuAdvancedIncSubtensor1_dev20{inplace=True, set_instead_of_inc=False}(sumSqGrads_embedding, GpuElemwise{sqr,no_inplace}.0, GpuReshape{1}.0)
   3.1%    31.5%       2.418s       3.32e-05s   72739    54   GpuDot22(GpuReshape{2}.0, W_hiddenLayer)
   3.1%    34.7%       2.404s       3.30e-05s   72739   118   GpuAdvancedIncSubtensor1_dev20{inplace=True, set_instead_of_inc=False}(embedding, GpuElemwise{Composite{(i0 * i1 * (i2 / (i3 + sqrt((i4 + i5)))))}}[(0, 2)]<gpuarray>.0, GpuReshape{1}.0)
   2.9%    37.5%       2.206s       3.03e-05s   72739   109   GpuDot22(GpuElemwise{Composite{(i0 * (i1 - sqr(tanh(i2))))}}[(0, 0)]<gpuarray>.0, InplaceGpuDimShuffle{1,0}.0)
   2.6%    40.1%       2.002s       2.75e-05s   72739    41   GpuAdvancedSubtensor1(embedding, GpuContiguous.0)
   2.2%    42.4%       1.726s       2.37e-05s   72739    39   GpuAdvancedSubtensor1(embedding, GpuContiguous.0)
   2.2%    44.6%       1.698s       2.33e-05s   72739    42   GpuAdvancedSubtensor1(sumSqGrads_embedding, GpuContiguous.0)
   2.1%    46.7%       1.603s       2.20e-05s   72739    40   GpuAdvancedSubtensor1(sumSqGrads_embedding, GpuContiguous.0)
   2.1%    48.8%       1.595s       2.19e-05s   72739    64   GpuDot22(GpuJoin.0, W_hiddenLayer)
   2.0%    50.7%       1.517s       2.09e-05s   72739    81   GpuDot22(InplaceGpuDimShuffle{1,0}.0, GpuCrossentropySoftmax1HotWithBiasDx.0)
   2.0%    52.7%       1.515s       2.08e-05s   72739    89   GpuDot22(GpuElemwise{Composite{(i0 * (i1 - sqr(i2)))}}[(0, 0)]<gpuarray>.0, InplaceGpuDimShuffle{1,0}.0)
   1.9%    54.7%       1.490s       2.05e-05s   72739    88   GpuDot22(InplaceGpuDimShuffle{1,0}.0, GpuElemwise{Composite{(i0 * (i1 - sqr(i2)))}}[(0, 0)]<gpuarray>.0)
   1.6%    56.2%       1.203s       1.65e-05s   72739    63   GpuJoin(TensorConstant{1}, GpuReshape{2}.0, GpuReshape{2}.0)
   1.6%    57.8%       1.200s       1.65e-05s   72739    96   GpuElemwise{Composite{(i0 - (i1 * (i2 / (i3 + sqrt(i4)))))}}[(0, 0)]<gpuarray>(W_hiddenLayer, InplaceGpuDimShuffle{x,x}.0, GpuDot22.0, GpuArrayConstant{[[  1.00000001e-10]]}, GpuElemwise{Composite{(i0 + sqr(i1))}}[(0, 0)]<gpuarray>.0)
   1.5%    59.3%       1.185s       1.63e-05s   72739    67   GpuDot22(GpuElemwise{Composite{tanh((i0 + i1))}}[(0, 0)]<gpuarray>.0, W_hiddenLayer)
   1.4%    60.7%       1.063s       1.46e-05s   72739    69   GpuCrossentropySoftmaxArgmax1HotWithBias(GpuDot22.0, b_hiddenLayer, GpuFromHost<None>.0)
   1.3%    62.0%       1.003s       1.38e-05s   72739    60   GpuCAReduceCuda{maximum}{1}(GpuElemwise{tanh,no_inplace}.0)
   ... (remaining 99 Apply instances account for 37.98%(29.16s) of the runtime)

Here are tips to potentially make your code run faster
                 (if you think of new ones, suggest them on the mailing list).
                 Test them first, as they are not guaranteed to always provide a speedup.
  - Try installing amdlibm and set the Theano flag lib.amdlibm=True. This speeds up only some Elemwise operation.
Function profiling
==================
  Message: /home/eraldo/lia-pln-deeplearning/model/Model.py:73
  Time in 0 calls to Function.__call__: 0.000000e+00s
  Total compile time: 6.361500e+00s
    Number of Apply nodes: 52
    Theano Optimizer time: 1.776628e-01s
       Theano validate time: 3.467321e-03s
    Theano Linker time (includes C, CUDA code generation/compiling): 6.171894e+00s
       Import time 5.018711e-03s
       Node make_thunk time 6.169770e+00s
           Node GpuElemwise{Tanh}[(0, 0)]<gpuarray>(GpuReshape{3}.0) time 1.393837e+00s
           Node GpuElemwise{Composite{tanh((i0 + i1))}}[(0, 0)]<gpuarray>(GpuDot22.0, InplaceGpuDimShuffle{x,0}.0) time 9.991140e-01s
           Node GpuCAReduceCuda{maximum}{1}(GpuElemwise{Tanh}[(0, 0)]<gpuarray>.0) time 9.913189e-01s
           Node GpuElemwise{Add}[(0, 0)]<gpuarray>(GpuDot22.0, InplaceGpuDimShuffle{x,0}.0) time 9.837110e-01s
           Node GpuCAReduceCuda{add}(GpuCrossentropySoftmaxArgmax1HotWithBias.0) time 6.499870e-01s

Time in all call to theano.grad() 7.505751e-02s
Time since theano import 177.252s
Here are tips to potentially make your code run faster
                 (if you think of new ones, suggest them on the mailing list).
                 Test them first, as they are not guaranteed to always provide a speedup.
  Sorry, no tip for today.
Function profiling
==================
  Message: Sum of all(2) printed profiles at exit excluding Scan op profile.
  Time in 72739 calls to Function.__call__: 8.634016e+01s
  Time in Function.fn.__call__: 8.301838e+01s (96.153%)
  Time in thunks: 7.678292e+01s (88.931%)
  Total compile time: 1.992799e+01s
    Number of Apply nodes: 119
    Theano Optimizer time: 1.026707e+00s
       Theano validate time: 3.767490e-02s
    Theano Linker time (includes C, CUDA code generation/compiling): 1.885781e+01s
       Import time 1.102138e-02s
       Node make_thunk time 1.884921e+01s
           Node GpuElemwise{tanh,no_inplace}(GpuReshape{3}.0) time 1.397602e+00s
           Node GpuElemwise{Tanh}[(0, 0)]<gpuarray>(GpuReshape{3}.0) time 1.393837e+00s
           Node GpuElemwise{Composite{(EQ(i0, i1) * i2)}}[(0, 1)]<gpuarray>(InplaceGpuDimShuffle{0,x,1}.0, GpuElemwise{tanh,no_inplace}.0, InplaceGpuDimShuffle{0,x,1}.0) time 1.379394e+00s
           Node GpuElemwise{Composite{tanh((i0 + i1))}}[(0, 0)]<gpuarray>(GpuDot22.0, InplaceGpuDimShuffle{x,0}.0) time 9.991140e-01s
           Node GpuElemwise{Composite{(i0 * (i1 - sqr(tanh(i2))))}}[(0, 0)]<gpuarray>(GpuReshape{2}.0, GpuArrayConstant{[[ 1.]]}, GpuElemwise{Add}[(0, 0)]<gpuarray>.0) time 9.976521e-01s

Time in all call to theano.grad() 7.505751e-02s
Time since theano import 177.252s
Class
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Class name>
  26.0%    26.0%      19.931s       3.04e-05s     C   654651       9   theano.gpuarray.blas.GpuDot22
  22.7%    48.7%      17.435s       8.88e-06s     C   1963953      27   theano.gpuarray.elemwise.GpuElemwise
  18.7%    67.3%      14.332s       3.94e-05s     C   363695       5   theano.gpuarray.elemwise.GpuCAReduceCuda
   9.2%    76.5%       7.029s       2.42e-05s     C   290956       4   theano.gpuarray.subtensor.GpuAdvancedSubtensor1
   7.9%    84.4%       6.062s       2.08e-05s     C   290956       4   theano.gpuarray.subtensor.GpuAdvancedIncSubtensor1_dev20
   4.0%    88.3%       3.042s       6.97e-06s     C   436434       6   theano.gpuarray.basic_ops.GpuFromHost
   2.6%    90.9%       1.995s       2.74e-06s     C   727390      10   theano.gpuarray.basic_ops.GpuReshape
   2.0%    92.9%       1.522s       6.97e-06s     C   218217       3   theano.gpuarray.basic_ops.HostFromGpu
   1.7%    94.6%       1.317s       1.29e-06s     C   1018346      14   theano.gpuarray.elemwise.GpuDimShuffle
   1.6%    96.2%       1.203s       1.65e-05s     C    72739       1   theano.gpuarray.basic_ops.GpuJoin
   1.4%    97.6%       1.063s       1.46e-05s     C    72739       1   theano.gpuarray.nnet.GpuCrossentropySoftmaxArgmax1HotWithBias
   0.8%    98.4%       0.649s       8.93e-06s     C    72739       1   theano.gpuarray.nnet.GpuCrossentropySoftmax1HotWithBiasDx
   0.5%    98.9%       0.371s       4.64e-07s     C   800129      11   theano.tensor.elemwise.Elemwise
   0.4%    99.3%       0.295s       4.06e-07s     C   727390      10   theano.compile.ops.Shape_i
   0.3%    99.6%       0.235s       3.59e-07s     C   654651       9   theano.tensor.opt.MakeVector
   0.2%    99.8%       0.174s       2.40e-06s     C    72739       1   theano.gpuarray.basic_ops.GpuSplit
   0.1%   100.0%       0.099s       1.36e-06s     C    72739       1   theano.tensor.basic.Argmax
   0.0%   100.0%       0.029s       1.96e-07s     C   145478       2   theano.gpuarray.basic_ops.GpuContiguous
   ... (remaining 0 Classes account for   0.00%(0.00s) of the runtime)

Ops
---
<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Op name>
  26.0%    26.0%      19.931s       3.04e-05s     C     654651        9   GpuDot22
  16.5%    42.5%      12.679s       5.81e-05s     C     218217        3   GpuCAReduceCuda{add}{0}
   9.2%    51.6%       7.029s       2.42e-05s     C     290956        4   GpuAdvancedSubtensor1
   7.9%    59.5%       6.062s       2.08e-05s     C     290956        4   GpuAdvancedIncSubtensor1_dev20{inplace=True, set_instead_of_inc=False}
   5.4%    64.9%       4.118s       9.44e-06s     C     436434        6   GpuElemwise{Composite{(i0 - (i1 * (i2 / (i3 + sqrt(i4)))))}}[(0, 0)]<gpuarray>
   4.3%    69.2%       3.337s       7.65e-06s     C     436434        6   GpuElemwise{Composite{(i0 + sqr(i1))}}[(0, 0)]<gpuarray>
   4.0%    73.2%       3.042s       6.97e-06s     C     436434        6   GpuFromHost<None>
   2.3%    75.5%       1.742s       3.42e-06s     C     509173        7   GpuReshape{2}
   2.0%    77.4%       1.522s       6.97e-06s     C     218217        3   HostFromGpu(gpuarray)
   1.9%    79.4%       1.489s       1.02e-05s     C     145478        2   GpuElemwise{Composite{(i0 * i1 * (i2 / (i3 + sqrt((i4 + i5)))))}}[(0, 2)]<gpuarray>
   1.8%    81.2%       1.417s       9.74e-06s     C     145478        2   GpuElemwise{Add}[(0, 0)]<gpuarray>
   1.7%    82.9%       1.298s       8.92e-06s     C     145478        2   GpuElemwise{sqr,no_inplace}
   1.6%    84.5%       1.203s       1.65e-05s     C     72739        1   GpuJoin
   1.4%    85.9%       1.063s       1.46e-05s     C     72739        1   GpuCrossentropySoftmaxArgmax1HotWithBias
   1.3%    87.2%       1.003s       1.38e-05s     C     72739        1   GpuCAReduceCuda{maximum}{1}
   1.0%    88.2%       0.766s       1.05e-05s     C     72739        1   GpuElemwise{tanh,no_inplace}
   1.0%    89.2%       0.763s       1.05e-05s     C     72739        1   GpuElemwise{Composite{(EQ(i0, i1) * i2)}}[(0, 1)]<gpuarray>
   0.9%    90.1%       0.729s       1.00e-05s     C     72739        1   GpuElemwise{Composite{(i0 * (i1 - sqr(tanh(i2))))}}[(0, 0)]<gpuarray>
   0.8%    91.0%       0.650s       8.94e-06s     C     72739        1   GpuCAReduceCuda{add}
   0.8%    91.8%       0.649s       8.93e-06s     C     72739        1   GpuCrossentropySoftmax1HotWithBiasDx
   ... (remaining 29 Ops account for   8.19%(6.29s) of the runtime)

Apply
------
<% time> <sum %> <apply time> <time per call> <#call> <id> <Apply name>
  14.7%    14.7%      11.293s       1.55e-04s   72739   107   GpuCAReduceCuda{add}{0}(GpuElemwise{Composite{(i0 * (i1 - sqr(tanh(i2))))}}[(0, 0)]<gpuarray>.0)
   5.3%    20.0%       4.084s       5.61e-05s   72739   108   GpuDot22(InplaceGpuDimShuffle{1,0}.0, GpuElemwise{Composite{(i0 * (i1 - sqr(tanh(i2))))}}[(0, 0)]<gpuarray>.0)
   5.1%    25.1%       3.920s       5.39e-05s   72739    79   GpuDot22(GpuCrossentropySoftmax1HotWithBiasDx.0, InplaceGpuDimShuffle{1,0}.0)
   3.2%    28.4%       2.495s       3.43e-05s   72739   116   GpuAdvancedIncSubtensor1_dev20{inplace=True, set_instead_of_inc=False}(sumSqGrads_embedding, GpuElemwise{sqr,no_inplace}.0, GpuReshape{1}.0)
   3.1%    31.5%       2.418s       3.32e-05s   72739    54   GpuDot22(GpuReshape{2}.0, W_hiddenLayer)
   3.1%    34.7%       2.404s       3.30e-05s   72739   118   GpuAdvancedIncSubtensor1_dev20{inplace=True, set_instead_of_inc=False}(embedding, GpuElemwise{Composite{(i0 * i1 * (i2 / (i3 + sqrt((i4 + i5)))))}}[(0, 2)]<gpuarray>.0, GpuReshape{1}.0)
   2.9%    37.5%       2.206s       3.03e-05s   72739   109   GpuDot22(GpuElemwise{Composite{(i0 * (i1 - sqr(tanh(i2))))}}[(0, 0)]<gpuarray>.0, InplaceGpuDimShuffle{1,0}.0)
   2.6%    40.1%       2.002s       2.75e-05s   72739    41   GpuAdvancedSubtensor1(embedding, GpuContiguous.0)
   2.2%    42.4%       1.726s       2.37e-05s   72739    39   GpuAdvancedSubtensor1(embedding, GpuContiguous.0)
   2.2%    44.6%       1.698s       2.33e-05s   72739    42   GpuAdvancedSubtensor1(sumSqGrads_embedding, GpuContiguous.0)
   2.1%    46.7%       1.603s       2.20e-05s   72739    40   GpuAdvancedSubtensor1(sumSqGrads_embedding, GpuContiguous.0)
   2.1%    48.8%       1.595s       2.19e-05s   72739    64   GpuDot22(GpuJoin.0, W_hiddenLayer)
   2.0%    50.7%       1.517s       2.09e-05s   72739    81   GpuDot22(InplaceGpuDimShuffle{1,0}.0, GpuCrossentropySoftmax1HotWithBiasDx.0)
   2.0%    52.7%       1.515s       2.08e-05s   72739    89   GpuDot22(GpuElemwise{Composite{(i0 * (i1 - sqr(i2)))}}[(0, 0)]<gpuarray>.0, InplaceGpuDimShuffle{1,0}.0)
   1.9%    54.7%       1.490s       2.05e-05s   72739    88   GpuDot22(InplaceGpuDimShuffle{1,0}.0, GpuElemwise{Composite{(i0 * (i1 - sqr(i2)))}}[(0, 0)]<gpuarray>.0)
   1.6%    56.2%       1.203s       1.65e-05s   72739    63   GpuJoin(TensorConstant{1}, GpuReshape{2}.0, GpuReshape{2}.0)
   1.6%    57.8%       1.200s       1.65e-05s   72739    96   GpuElemwise{Composite{(i0 - (i1 * (i2 / (i3 + sqrt(i4)))))}}[(0, 0)]<gpuarray>(W_hiddenLayer, InplaceGpuDimShuffle{x,x}.0, GpuDot22.0, GpuArrayConstant{[[  1.00000001e-10]]}, GpuElemwise{Composite{(i0 + sqr(i1))}}[(0, 0)]<gpuarray>.0)
   1.5%    59.3%       1.185s       1.63e-05s   72739    67   GpuDot22(GpuElemwise{Composite{tanh((i0 + i1))}}[(0, 0)]<gpuarray>.0, W_hiddenLayer)
   1.4%    60.7%       1.063s       1.46e-05s   72739    69   GpuCrossentropySoftmaxArgmax1HotWithBias(GpuDot22.0, b_hiddenLayer, GpuFromHost<None>.0)
   1.3%    62.0%       1.003s       1.38e-05s   72739    60   GpuCAReduceCuda{maximum}{1}(GpuElemwise{tanh,no_inplace}.0)
   ... (remaining 99 Apply instances account for 37.98%(29.16s) of the runtime)

Here are tips to potentially make your code run faster
                 (if you think of new ones, suggest them on the mailing list).
                 Test them first, as they are not guaranteed to always provide a speedup.
  - Try installing amdlibm and set the Theano flag lib.amdlibm=True. This speeds up only some Elemwise operation.
Skipping empty Profile
Skipping empty Profile
